# apply_milestones_5_6.py
"""
One-shot writer for Milestones 5 & 6.
- Creates/updates:
  - pyproject.toml
  - backend/obs/events.py, backend/obs/middleware.py
  - backend/services/history.py, backend/services/mcp_servers.py
  - backend/services/orchestrator.py (UPDATED)
  - backend/api/main.py (UPDATED)
  - ui/app.py (UPDATED)
  - README.md (appends a Milestones 5‚Äì6 section if it exists, else creates a fresh README)

Run from the project root:
    python apply_milestones_5_6.py
"""

from __future__ import annotations
from pathlib import Path
import sys

ROOT = Path.cwd()

FILES = {
    # -------------------- root files --------------------
    "pyproject.toml": r'''[tool.ruff]
line-length = 100
target-version = "py311"
select = ["E","F","I","UP"]
ignore = []

[tool.pytest.ini_options]
addopts = "-q"
testpaths = ["tests"]
pythonpath = ["."]
''',

    # -------------------- backend/obs --------------------
    "backend/obs/__init__.py": r'''# package init
''',

    "backend/obs/events.py": r'''"""
Lightweight event sink ‚Üí JSONL at backend/data/logs/events.jsonl
"""
from __future__ import annotations
from pathlib import Path
import json
import time
from typing import Any, Dict

LOG_DIR = Path(__file__).resolve().parents[2] / "backend" / "data" / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOG_DIR / "events.jsonl"

def record_event(kind: str, payload: Dict[str, Any]) -> None:
    rec = {
        "ts": time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime()),
        "kind": kind,
        "payload": payload,
    }
    with LOG_FILE.open("a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
''',

    "backend/obs/middleware.py": r'''"""
FastAPI middleware that logs basic request/response data to events.jsonl
"""
from __future__ import annotations
import time
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response
from .events import record_event

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        t0 = time.perf_counter()
        path = request.url.path
        method = request.method
        try:
            response: Response = await call_next(request)
            return response
        finally:
            dt = (time.perf_counter() - t0) * 1000.0
            record_event("http", {"path": path, "method": method, "ms": round(dt, 2)})
''',

    # -------------------- backend/services --------------------
    "backend/services/history.py": r'''"""
Q&A history store (in-memory). Keeps recent records for conversation explorer.
"""
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Any
from time import time

@dataclass
class QARecord:
    ts: float
    question: str
    answer: str
    sources: List[str]
    validation: Dict[str, Any] | None

class QAHistory:
    def __init__(self, max_items: int = 200) -> None:
        self._items: List[QARecord] = []
        self._max = max_items

    def add(self, *, question: str, answer: str, sources: List[str], validation: Dict[str, Any] | None) -> None:
        self._items.append(QARecord(ts=time(), question=question, answer=answer, sources=sources, validation=validation))
        if len(self._items) > self._max:
            self._items = self._items[-self._max:]

    def list(self, last: int = 50) -> List[Dict[str, Any]]:
        out = []
        for r in self._items[-last:][::-1]:
            out.append({
                "ts": r.ts,
                "question": r.question,
                "answer": r.answer,
                "sources": r.sources,
                "validation": r.validation,
            })
        return out

qa_history = QAHistory()
''',

    "backend/services/mcp_servers.py": r'''"""
Tiny "MCP-like" servers (local stubs):
- file_ops: list/read/write files in uploads/
- search: keyword search over uploaded text (simple contains or vector search)
"""
from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Any
from .vector_store import VectorStore
from .parser import parse_file

UPLOADS = Path(__file__).resolve().parents[1] / "data" / "uploads"

class MCPFileOps:
    @staticmethod
    def list_files() -> List[str]:
        if not UPLOADS.exists():
            return []
        return sorted([p.name for p in UPLOADS.iterdir() if p.is_file()])

    @staticmethod
    def read_file(name: str) -> str:
        p = UPLOADS / name
        if not p.exists() or not p.is_file():
            raise FileNotFoundError(name)
        return parse_file(str(p), p.suffix.lower())

    @staticmethod
    def write_text(name: str, content: str) -> str:
        p = UPLOADS / name
        p.write_text(content, encoding="utf-8")
        return str(p)

class MCPSearch:
    def __init__(self, vstore: VectorStore) -> None:
        self.vs = vstore

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        rows = self.vs.search(query, top_k=top_k)
        return [{"doc_id": d, "chunk_idx": i, "score": s, "snippet": c[:300]} for (d, i, s, c) in rows]
''',

    # -------------------- backend/services/orchestrator.py (UPDATED) --------------------
    "backend/services/orchestrator.py": r'''from pathlib import Path
import time
from .document_store import DocumentStore
from ..core.logger import get_logger
from ..core.config import settings
from .parser import parse_file
from .chunker import chunk_text
from .vector_store import VectorStore
from .summary_agent import SummaryAgent
from .entity_extraction import extract_entities
from .qa_agent import QAAgent
from .validator_agent import validate_summary, validate_answer
from .history import qa_history
from ..obs.events import record_event

log = get_logger("orchestrator")

class Orchestrator:
    def __init__(self, store: DocumentStore) -> None:
        self.store = store
        self.vstore = VectorStore(dim=384)
        self.summarizer = SummaryAgent()
        self.qa = QAAgent(self.vstore)
        self._entities: dict[str, dict] = {}
        self.default_summary_words = getattr(settings, "summary_words_default", 350)

    def _reparse_chunks(self, saved_path: str) -> list[str]:
        ext = Path(saved_path).suffix.lower()
        text = parse_file(saved_path, ext)
        return chunk_text(text)

    def ingest_document(self, *, filename: str, saved_path: str) -> str:
        doc_id = self.store.add_document(filename=filename, path=saved_path)
        log.info("Document registered: %s", doc_id)
        try:
            chunks = self._reparse_chunks(saved_path)
            self.vstore.upsert_document(doc_id, chunks)

            summary = self.summarizer.summarize(chunks, target_words=self.default_summary_words)
            val = validate_summary(summary, min_words=int(self.default_summary_words*0.6), max_words=int(self.default_summary_words*1.6))
            self.store.push_summary_version(doc_id, summary, note="ingest_summary", validation=val)

            entities = extract_entities(summary)
            self._entities[doc_id] = entities
            log.info("Ingest complete: %s", doc_id)
            record_event("ingest", {"doc_id": doc_id, "filename": filename})
        except Exception as e:
            log.exception("Processing error")
            self.store.set_error(doc_id, f"Processing error: {e}")
            record_event("error", {"doc_id": doc_id, "filename": filename, "error": str(e)})
        return doc_id

    def generate_summary(self, *, document_id: str, target_words: int, mode: str = "extractive_mmr", temperature: float = 0.2, seed: int | None = None) -> dict:
        doc = self.store.get(document_id)
        if not doc:
            return {"ok": False, "error": "not_found"}
        try:
            chunks = self._reparse_chunks(doc.path)
            self.vstore.upsert_document(document_id, chunks)

            cur_summary = doc.summary or ""
            ents = extract_entities(cur_summary) if cur_summary else {"names": [], "dates": [], "organizations": []}
            seed_val = int(seed) if seed is not None else int(time.time() * 1000) % 1_000_000

            summary = self.summarizer.summarize(
                chunks,
                target_words=target_words,
                mode=mode,
                temperature=temperature,
                seed=seed_val,
                entities=(ents.get("names", []), ents.get("dates", []), ents.get("organizations", [])),
            )
            val = validate_summary(summary, min_words=int(target_words*0.6), max_words=int(target_words*1.6))
            self.store.push_summary_version(document_id, summary, note=f"regen_{mode}_{target_words}_t{temperature}_seed{seed_val}", validation=val)
            self._entities[document_id] = extract_entities(summary)
            record_event("summary_regen", {"doc_id": document_id, "mode": mode, "seed": seed_val})
            return {"ok": True, "validation": val, "summary": summary, "seed": seed_val}
        except Exception as e:
            log.exception("Regeneration error")
            self.store.set_error(document_id, f"Processing error: {e}")
            record_event("error", {"doc_id": document_id, "error": str(e)})
            return {"ok": False, "error": "processing_error"}

    def get_summary(self, *, document_id: str) -> dict:
        doc = self.store.get(document_id)
        if not doc:
            return {"status": "not_found", "summary": None}
        return {"status": doc.status, "summary": doc.summary}

    def save_summary(self, *, document_id: str, summary: str) -> dict:
        doc = self.store.get(document_id)
        if not doc:
            return {"ok": False, "error": "not_found"}
        val = validate_summary(summary, min_words=int(self.default_summary_words*0.6), max_words=int(self.default_summary_words*1.6))
        self.store.push_summary_version(document_id, summary, note="manual_save", validation=val)
        self._entities[document_id] = extract_entities(summary)
        record_event("summary_save", {"doc_id": document_id})
        return {"ok": True, "validation": val}

    def validate_current_summary(self, *, document_id: str) -> dict:
        doc = self.store.get(document_id)
        if not doc or not doc.summary:
            return {"ok": False, "error": "not_found_or_empty"}
        val = validate_summary(doc.summary, min_words=int(self.default_summary_words*0.6), max_words=int(self.default_summary_words*1.6))
        return {"ok": True, "validation": val}

    def list_summary_versions(self, *, document_id: str) -> list[dict]:
        doc = self.store.get(document_id)
        if not doc:
            return []
        out = []
        for idx, v in enumerate(self.store.list_summary_versions(document_id)):
            out.append({
                "index": idx,
                "created_at": v.created_at,
                "note": v.note,
                "validation": v.validation,
                "word_count": v.validation.get("word_count") if v.validation else None
            })
        return out

    def rollback_summary(self, *, document_id: str, version_index: int) -> dict:
        ok = self.store.rollback_summary(document_id, version_index)
        if ok:
            record_event("summary_rollback", {"doc_id": document_id, "version_index": version_index})
        return {"ok": ok}

    def get_entities(self, *, document_id: str) -> dict:
        if document_id in self._entities:
            return {"status": "ready", "entities": self._entities[document_id]}
        doc = self.store.get(document_id)
        if not doc or not doc.summary:
            return {"status": "pending", "entities": None}
        ents = extract_entities(doc.summary)
        self._entities[document_id] = ents
        return {"status": "ready", "entities": ents}

    def save_entities(self, *, document_id: str, entities: dict) -> dict:
        self._entities[document_id] = entities
        record_event("entities_save", {"doc_id": document_id})
        return {"ok": True}

    def answer_question(self, *, question: str, document_ids: list[str] | None) -> dict:
        result = self.qa.answer(question=question, top_k=5)
        val = validate_answer(result["answer"], result.get("contexts", []))
        result["validation"] = val
        qa_history.add(question=question, answer=result["answer"], sources=result.get("sources", []), validation=val)
        record_event("qa", {"question": question, "sources": result.get("sources", []), "score": val.get("score")})
        return result
''',

    # -------------------- backend/api/main.py (UPDATED) --------------------
    "backend/api/main.py": r'''from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import PlainTextResponse, JSONResponse
from pydantic import BaseModel, Field
from pathlib import Path
import shutil
from typing import Any, List

from ..core.config import settings
from ..core.logger import get_logger
from ..services.document_store import DocumentStore
from ..services.orchestrator import Orchestrator
from ..obs.middleware import RequestLoggingMiddleware
from ..obs.events import record_event
from ..services.history import qa_history
from ..services.mcp_servers import MCPFileOps, MCPSearch

log = get_logger("api")

app = FastAPI(title="Intelligent Docs API", version="0.5.0")

# Middleware: CORS + Request logging
app.add_middleware(
    CORSMiddleware,
    allow_origins=[o.strip() for o in settings.cors_origins],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(RequestLoggingMiddleware)

# Global exception handler
@app.exception_handler(Exception)
async def unhandled_exc(request: Request, exc: Exception):
    record_event("error", {"path": request.url.path, "error": str(exc)})
    return JSONResponse(status_code=500, content={"detail": "internal_error"})

UPLOAD_DIR = Path(__file__).resolve().parent.parent / "data" / "uploads"
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

store = DocumentStore()
orchestrator = Orchestrator(store=store)

class DocumentCreateResponse(BaseModel):
    document_id: str
    filename: str
    status: str = "pending"

class SummaryResponse(BaseModel):
    document_id: str
    status: str = Field(description="pending | ready | error | not_found")
    summary: str | None = None

class SummarySaveRequest(BaseModel):
    summary: str

class SummarizeRequest(BaseModel):
    target_words: int = Field(ge=50, le=1200, default=getattr(settings, "summary_words_default", 350))
    mode: str = Field(default="extractive_mmr")  # or "abstractive"
    temperature: float = Field(ge=0.0, le=1.0, default=0.2)
    seed: int | None = None

class EntitiesResponse(BaseModel):
    status: str
    entities: dict[str, Any] | None = None

class EntitiesSaveRequest(BaseModel):
    entities: dict[str, Any]

class QARequest(BaseModel):
    question: str
    document_ids: List[str] | None = None

class QAResponse(BaseModel):
    answer: str
    sources: List[str] = []
    validation: dict | None = None

class VersionsResponseItem(BaseModel):
    index: int
    created_at: str
    note: str | None = None
    validation: dict | None = None
    word_count: int | None = None

SUPPORTED_TYPES = {".pdf", ".docx", ".txt", ".html", ".htm"}

@app.post("/documents", response_model=DocumentCreateResponse)
async def upload_document(file: UploadFile = File(...)):
    ext = Path(file.filename).suffix.lower()
    if ext not in SUPPORTED_TYPES:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: {ext}")

    dest = UPLOAD_DIR / file.filename
    with dest.open("wb") as f:
        shutil.copyfileobj(file.file, f)

    document_id = orchestrator.ingest_document(filename=file.filename, saved_path=str(dest))
    status = store.get(document_id).status
    return DocumentCreateResponse(document_id=document_id, filename=file.filename, status=status)

@app.get("/documents/{document_id}/summary", response_model=SummaryResponse)
async def get_summary(document_id: str):
    result = orchestrator.get_summary(document_id=document_id)
    return SummaryResponse(document_id=document_id, **result)

@app.post("/documents/{document_id}/summary")
async def save_summary(document_id: str, req: SummarySaveRequest):
    result = orchestrator.save_summary(document_id=document_id, summary=req.summary)
    if not result.get("ok"):
        raise HTTPException(status_code=404, detail="Document not found")
    return {"ok": True, "validation": result.get("validation")}

@app.post("/documents/{document_id}/summarize")
async def regenerate_summary(document_id: str, req: SummarizeRequest):
    res = orchestrator.generate_summary(
        document_id=document_id,
        target_words=req.target_words,
        mode=req.mode,
        temperature=req.temperature,
        seed=req.seed,
    )
    if not res.get("ok"):
        raise HTTPException(status_code=404, detail=res.get("error","regenerate_failed"))
    return {"ok": True, "validation": res.get("validation"), "summary": res.get("summary"), "seed": res.get("seed")}

@app.post("/documents/{document_id}/summary/validate")
async def validate_summary_route(document_id: str):
    res = orchestrator.validate_current_summary(document_id=document_id)
    if not res.get("ok"):
        raise HTTPException(status_code=404, detail=res.get("error","not_found"))
    return {"ok": True, "validation": res["validation"]}

@app.get("/documents/{document_id}/summary/versions", response_model=List[VersionsResponseItem])
async def list_summary_versions(document_id: str):
    items = orchestrator.list_summary_versions(document_id=document_id)
    return [VersionsResponseItem(**it) for it in items]

@app.post("/documents/{document_id}/summary/rollback")
async def rollback_summary(document_id: str, version_index: int = 0):
    res = orchestrator.rollback_summary(document_id=document_id, version_index=version_index)
    if not res.get("ok"):
        raise HTTPException(status_code=404, detail="rollback_failed")
    return {"ok": True}

@app.get("/documents/{document_id}/entities", response_model=EntitiesResponse)
async def get_entities(document_id: str):
    return orchestrator.get_entities(document_id=document_id)

@app.post("/documents/{document_id}/entities")
async def save_entities(document_id: str, req: EntitiesSaveRequest):
    return orchestrator.save_entities(document_id=document_id, entities=req.entities)

@app.post("/qa", response_model=QAResponse)
async def qa(request: QARequest):
    result = orchestrator.answer_question(question=request.question, document_ids=request.document_ids)
    return QAResponse(answer=result["answer"], sources=result.get("sources", []), validation=result.get("validation"))

@app.get("/qa/history")
async def qa_history_list(limit: int = 50):
    return {"items": qa_history.list(last=limit)}

@app.get("/documents/{document_id}/export/summary.md")
async def export_summary_md(document_id: str):
    res = orchestrator.get_summary(document_id=document_id)
    if res["status"] != "ready" or not res["summary"]:
        raise HTTPException(status_code=404, detail="summary_not_ready")
    return PlainTextResponse(res["summary"], media_type="text/markdown")

@app.get("/documents/{document_id}/export/entities.json")
async def export_entities_json(document_id: str):
    ents = orchestrator.get_entities(document_id=document_id)
    if ents["status"] != "ready":
        raise HTTPException(status_code=404, detail="entities_not_ready")
    return JSONResponse(ents["entities"])

# -------- MCP (stub) --------
@app.get("/mcp/files")
async def mcp_list_files():
    return {"files": MCPFileOps.list_files()}

@app.get("/mcp/files/read")
async def mcp_read_file(name: str):
    try:
        content = MCPFileOps.read_file(name)
        return {"name": name, "content": content[:20000]}  # cap
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="file_not_found")

@app.post("/mcp/files/write")
async def mcp_write_file(name: str, content: str):
    path = MCPFileOps.write_text(name, content)
    return {"ok": True, "path": path}

_search = MCPSearch(orchestrator.vstore)

@app.get("/mcp/search")
async def mcp_search(q: str, k: int = 5):
    return {"results": _search.search(q, top_k=k)}

@app.get("/health")
async def health():
    return {"ok": True, "env": settings.app_env}
''',

    # -------------------- ui/app.py (UPDATED) --------------------
    "ui/app.py": r'''import streamlit as st
import requests
import os
import json

API_BASE = f"http://{os.getenv('API_HOST','127.0.0.1')}:{os.getenv('API_PORT','8000')}"

st.set_page_config(page_title="Intelligent Document Summarization & Q&A", layout="wide")
st.title("üìÑ Intelligent Document Summarization & Q&A")

st.sidebar.header("Status & Settings")
st.sidebar.write(f"API: {API_BASE}")

# ---------------- Upload ----------------
st.header("1) Upload Document")
uploaded = st.file_uploader("Choose a file (.pdf, .docx, .txt, .html)", type=["pdf","docx","txt","html"])
if st.button("Upload", type="primary") and uploaded:
    files = {"file": (uploaded.name, uploaded.getvalue(), uploaded.type)}
    try:
        r = requests.post(f"{API_BASE}/documents", files=files, timeout=120)
        if r.status_code == 200:
            data = r.json()
            st.success(f"Uploaded ‚úÖ | id={data['document_id']} | status={data['status']}")
            st.session_state.setdefault("doc_ids", []).append(data["document_id"])
        else:
            st.error(f"Upload failed: {r.text}")
    except Exception as e:
        st.error(f"Error: {e}")

# ---------------- Summaries & Entities ----------------
st.divider()
st.header("2) Summaries & Entities")
doc_ids = st.session_state.get("doc_ids", [])
if doc_ids:
    selected = st.selectbox("Select a document id", doc_ids)
    target_words = st.slider("Target summary length (words)", min_value=100, max_value=800, value=350, step=50)
    mode = st.selectbox("Summary mode", options=["extractive_mmr", "abstractive"], index=0)
    temperature = st.slider("Temperature (LLM only)", min_value=0.0, max_value=1.0, value=0.2, step=0.1)
    seed = st.number_input("Seed (optional; leave 0 for random)", min_value=0, max_value=1_000_000, value=0, step=1)

    col1, col2 = st.columns(2)

    with col1:
        c1, c2, c3 = st.columns(3)
        with c1:
            if st.button("Get Summary"):
                try:
                    r = requests.get(f"{API_BASE}/documents/{selected}/summary", timeout=60)
                    if r.status_code == 200:
                        st.session_state["summary_data"] = r.json()
                    else:
                        st.error(r.text)
                except Exception as e:
                    st.error(f"Error: {e}")
        with c2:
            if st.button("Regenerate"):
                try:
                    payload = {
                        "target_words": target_words,
                        "mode": mode,
                        "temperature": float(temperature),
                    }
                    if seed and int(seed) > 0:
                        payload["seed"] = int(seed)
                    rr = requests.post(f"{API_BASE}/documents/{selected}/summarize", json=payload, timeout=120)
                    if rr.status_code == 200:
                        data = rr.json()
                        st.success(f"Regenerated (~{target_words} words). seed={data.get('seed')}")
                        st.session_state["summary_data"] = {
                            "document_id": selected,
                            "status": "ready",
                            "summary": data.get("summary", "")
                        }
                        st.session_state["summary_validation"] = data.get("validation")
                    else:
                        st.error(rr.text)
                except Exception as e:
                    st.error(f"Error: {e}")
        with c3:
            if st.button("Validate Summary"):
                try:
                    rv = requests.post(f"{API_BASE}/documents/{selected}/summary/validate", timeout=60)
                    if rv.status_code == 200:
                        st.session_state["summary_validation"] = rv.json()["validation"]
                        st.success("Validation complete.")
                    else:
                        st.error(rv.text)
                except Exception as e:
                    st.error(f"Error: {e}")

        sdata = st.session_state.get("summary_data")
        if sdata and sdata["document_id"] == selected:
            status = sdata["status"]
            if status == "ready":
                edited = st.text_area("Summary (editable):", sdata["summary"] or "", height=260)
                if st.button("Save Summary"):
                    try:
                        rr = requests.post(f"{API_BASE}/documents/{selected}/summary", json={"summary": edited}, timeout=60)
                        if rr.status_code == 200:
                            st.success("Summary saved.")
                            st.session_state["summary_data"]["summary"] = edited
                            st.session_state["summary_validation"] = rr.json().get("validation")
                        else:
                            st.error(rr.text)
                    except Exception as e:
                        st.error(f"Error: {e}")
            elif status == "pending":
                st.warning("Summary is pending.")
            elif status == "error":
                st.error(sdata["summary"] or "Processing error.")
            else:
                st.error("Document not found.")

        val = st.session_state.get("summary_validation")
        with st.expander("Summary Validation"):
            if val:
                st.write(val)
            else:
                st.caption("No validation run yet.")

        with st.expander("Summary Versions & Rollback"):
            try:
                lv = requests.get(f"{API_BASE}/documents/{selected}/summary/versions", timeout=60)
                if lv.status_code == 200:
                    versions = lv.json()
                    if versions:
                        st.table(versions)
                        idx = st.number_input("Rollback to version index", min_value=0, max_value=len(versions)-1, value=0, step=1)
                        if st.button("Rollback"):
                            rb = requests.post(f"{API_BASE}/documents/{selected}/summary/rollback", params={"version_index": int(idx)}, timeout=60)
                            if rb.status_code == 200:
                                st.success("Rolled back.")
                                r = requests.get(f"{API_BASE}/documents/{selected}/summary", timeout=60)
                                if r.status_code == 200:
                                    st.session_state["summary_data"] = r.json()
                            else:
                                st.error(rb.text)
                    else:
                        st.info("No versions yet.")
                else:
                    st.error(lv.text)
            except Exception as e:
                st.error(f"Error: {e}")

    with col2:
        if st.button("Get Entities"):
            try:
                r = requests.get(f"{API_BASE}/documents/{selected}/entities", timeout=60)
                if r.status_code == 200:
                    st.session_state["entities_data"] = r.json()
                else:
                    st.error(r.text)
            except Exception as e:
                st.error(f"Error: {e}")

        edata = st.session_state.get("entities_data")
        if edata and edata.get("status") == "ready":
            text = json.dumps(edata.get("entities", {}), indent=2, ensure_ascii=False)
            edited_entities = st.text_area("Entities (editable JSON):", text, height=260)
            if st.button("Save Entities"):
                try:
                    payload = json.loads(edited_entities)
                    rr = requests.post(f"{API_BASE}/documents/{selected}/entities", json={"entities": payload}, timeout=60)
                    if rr.status_code == 200:
                        st.success("Entities saved.")
                        st.session_state["entities_data"]["entities"] = payload
                    else:
                        st.error(rr.text)
                except Exception as e:
                    st.error(f"Invalid JSON or network error: {e}")
else:
    st.info("Upload a document first to view its summary and entities.")

# ---------------- Q&A ----------------
st.divider()
st.header("3) Ask Questions")
question = st.text_input("Your question")
use_docs = st.multiselect("Restrict to document ids (optional)", options=doc_ids, default=doc_ids)
if st.button("Ask"):
    try:
        r = requests.post(f"{API_BASE}/qa", json={"question": question, "document_ids": use_docs}, timeout=60)
        if r.status_code == 200:
            data = r.json()
            st.subheader("Answer")
            st.write(data["answer"])
            st.caption(f"Sources: {', '.join(data.get('sources', [])) or 'N/A'}")
            with st.expander("Answer Validation"):
                st.write(data.get("validation", {}))
        else:
            st.error(r.text)
    except Exception as e:
        st.error(f"Error: {e}")

# ---------------- History & Exports ----------------
st.divider()
st.header("4) History & Exports")

colA, colB = st.columns(2)
with colA:
    st.subheader("Q&A History")
    try:
        hr = requests.get(f"{API_BASE}/qa/history", params={"limit": 50}, timeout=30)
        if hr.status_code == 200:
            items = hr.json().get("items", [])
            if not items:
                st.caption("No Q&A yet.")
            else:
                for it in items:
                    with st.expander(f"Q: {it['question']}"):
                        st.write(it["answer"])
                        st.caption(f"Sources: {', '.join(it.get('sources', [])) or 'N/A'}")
                        st.code(it.get("validation", {}), language="json")
        else:
            st.error(hr.text)
    except Exception as e:
        st.error(f"Error: {e}")

with colB:
    st.subheader("Export")
    if doc_ids:
        selected_exp = st.selectbox("Document to export", doc_ids, key="export_doc")
        c1, c2 = st.columns(2)
        with c1:
            if st.button("Download Summary (.md)"):
                try:
                    rr = requests.get(f"{API_BASE}/documents/{selected_exp}/export/summary.md", timeout=30)
                    if rr.status_code == 200:
                        st.download_button("Save summary.md", data=rr.text, file_name="summary.md", mime="text/markdown")
                    else:
                        st.error(rr.text)
                except Exception as e:
                    st.error(f"Error: {e}")
        with c2:
            if st.button("Download Entities (.json)"):
                try:
                    rr = requests.get(f"{API_BASE}/documents/{selected_exp}/export/entities.json", timeout=30)
                    if rr.status_code == 200:
                        st.download_button("Save entities.json", data=rr.text, file_name="entities.json", mime="application/json")
                    else:
                        st.error(rr.text)
                except Exception as e:
                    st.error(f"Error: {e}")
    else:
        st.caption("Upload a document to enable exports.")
''',
}

README_SECTION = r'''# Intelligent Document Summarization & Q&A

Milestones 1‚Äì6:
- Upload docs (PDF/DOCX/TXT/HTML) ‚Üí parse ‚Üí chunk ‚Üí embed
- Summaries with target length, versions, validation, rollback
- Entities (names/dates/orgs) with clean extraction
- Q&A (retrieval; LLM optional) + validation
- **MCP-like stubs** (file ops + search), **observability logs**, **Q&A history**
- Exports: summary.md / entities.json
- Tests + lint config (ruff)

## Quick Start

```bash
python -m venv .venv
# Windows: .\.venv\Scripts\activate
# Linux/Mac: source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt

uvicorn backend.api.main:app --reload --host 127.0.0.1 --port 8000
streamlit run ui/app.py






/***********************************************
give what are steps to add the pgvecotrs and mcp sercersor
ChatGPT said:
Thought for 58s

Absolutely‚Äîhere‚Äôs a crisp, do-this-next guide to add pgvector (for persistent embeddings) and get your MCP servers (the lightweight ‚Äúfile ops + search‚Äù you already have) online. I‚Äôm including the exact env vars, requirements, DB SQL, and small code you need to drop in.

1) Add pgvector (Postgres vector store)
A) Install Postgres + pgvector

Option 1 (Docker, easiest):

docker run --name pg-docs -p 5432:5432 \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=docs \
  -v pgdocs:/var/lib/postgresql/data \
  ankane/pgvector


Option 2 (Local Postgres):

Install Postgres.

In psql:

CREATE DATABASE docs;
\c docs
CREATE EXTENSION IF NOT EXISTS vector;

B) Add env vars

In .env (and .env.example):

USE_PGVECTOR=1
DATABASE_URL=postgresql://postgres:postgres@127.0.0.1:5432/docs
PGVECTOR_DIM=384


PGVECTOR_DIM must match your embedder‚Äôs dimension (we use 384 in HashedEmbeddings).

C) Add lightweight deps (no strict pins)

In requirements.txt add:

psycopg[binary]
pgvector


You don‚Äôt need SQLAlchemy for this minimal setup.

D) Create tables & index (once)

Run in psql (connected to your docs DB):

CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE IF NOT EXISTS documents (
  id TEXT PRIMARY KEY,
  filename TEXT NOT NULL,
  path TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS chunks (
  doc_id TEXT NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
  chunk_idx INT NOT NULL,
  content TEXT NOT NULL,
  embedding VECTOR(384) NOT NULL,
  PRIMARY KEY (doc_id, chunk_idx)
);

-- Cosine distance, IVF index (fast + simple)
CREATE INDEX IF NOT EXISTS idx_chunks_embedding_ivf
  ON chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
ANALYZE chunks;


If your embed dimension differs, change VECTOR(384) accordingly (and keep PGVECTOR_DIM in sync).

E) Drop-in vector store for Postgres

Create backend/services/vector_store_pg.py:

# backend/services/vector_store_pg.py
from __future__ import annotations
import os
import math
import psycopg
from typing import List, Tuple
from .embeddings import HashedEmbeddings

def _l2_normalize(v: List[float]) -> List[float]:
    s = math.sqrt(sum(x*x for x in v)) or 1.0
    return [x / s for x in v]

def _vec_lit(v: List[float]) -> str:
    # vector literal for pgvector: '[0.1, 0.2, ...]'
    return "[" + ",".join(f"{x:.6f}" for x in v) + "]"

class PgVectorStore:
    """
    Minimal pgvector-backed store.
    - upsert_document(doc_id, chunks): deletes old rows and inserts new content+embeddings
    - search(query, top_k): cosine distance over normalized embeddings
    """
    def __init__(self, dsn: str | None = None, dim: int | None = None):
        self.dsn = dsn or os.getenv("DATABASE_URL")
        if not self.dsn:
            raise RuntimeError("DATABASE_URL not set")
        self.dim = int(dim or os.getenv("PGVECTOR_DIM", "384"))
        self.embedder = HashedEmbeddings(dim=self.dim)
        self.conn = psycopg.connect(self.dsn, autocommit=True)

        with self.conn.cursor() as cur:
            cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            # Tables should already be created by your migration SQL;
            # keeping idempotent creation here is optional.

    def upsert_document(self, doc_id: str, chunks: List[str], filename: str | None = None, path: str | None = None) -> None:
        embs = self.embedder.embed_texts(chunks)
        embs = [_l2_normalize(list(map(float, e))) for e in embs]

        with self.conn.cursor() as cur:
            if filename and path:
                cur.execute(
                    "INSERT INTO documents (id, filename, path) VALUES (%s, %s, %s) "
                    "ON CONFLICT (id) DO UPDATE SET filename=EXCLUDED.filename, path=EXCLUDED.path",
                    (doc_id, filename, path),
                )
            cur.execute("DELETE FROM chunks WHERE doc_id=%s", (doc_id,))
            for i, (content, emb) in enumerate(zip(chunks, embs)):
                cur.execute(
                    "INSERT INTO chunks (doc_id, chunk_idx, content, embedding) VALUES (%s, %s, %s, %s::vector)",
                    (doc_id, i, content, _vec_lit(emb)),
                )

    def delete_document(self, doc_id: str) -> None:
        with self.conn.cursor() as cur:
            cur.execute("DELETE FROM chunks WHERE doc_id=%s", (doc_id,))
            cur.execute("DELETE FROM documents WHERE id=%s", (doc_id,))

    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, int, float, str]]:
        qemb = self.embedder.embed_texts([query])[0]
        qemb = _l2_normalize(list(map(float, qemb)))
        with self.conn.cursor() as cur:
            # cosine distance operator `<=>`; smaller = closer. Convert to similarity (1 - dist).
            cur.execute(
                """
                SELECT doc_id, chunk_idx, content, (1 - (embedding <=> %s::vector)) AS sim
                FROM chunks
                ORDER BY embedding <=> %s::vector
                LIMIT %s
                """,
                (_vec_lit(qemb), _vec_lit(qemb), top_k),
            )
            rows = cur.fetchall()
        # return: (doc_id, chunk_idx, score, content)
        return [(r[0], int(r[1]), float(r[3]), r[2]) for r in rows]

F) Switch Orchestrator to pgvector when enabled

Edit backend/services/orchestrator.py:

# add at the top with other imports
import os
from .vector_store_pg import PgVectorStore
from .vector_store import VectorStore  # keep fallback

# in __init__
use_pg = os.getenv("USE_PGVECTOR", "0") == "1"
self.vstore = PgVectorStore() if use_pg else VectorStore(dim=384)


And when you ingest:

# after you save the uploaded file path
self.vstore.upsert_document(doc_id, chunks, filename=filename, path=saved_path)


(For the in-memory store the extra args are ignored; for PgVector they populate the documents table.)

That‚Äôs it‚Äîembeddings now persist in Postgres and your search scales beyond process memory.

2) Add MCP servers

You already have MCP-style HTTP endpoints (Milestone 5) at:

GET /mcp/files ‚Äî list uploaded files

GET /mcp/files/read?name= ‚Äî parse & return text

POST /mcp/files/write?name= ‚Äî write text to uploads

GET /mcp/search?q=&k= ‚Äî vector search over your corpus

A) Verify they work

Start API and test quickly:

curl "http://127.0.0.1:8000/mcp/files"
curl "http://127.0.0.1:8000/mcp/search?q=payment%20terms&k=5"


These are intentionally small and safe ‚ÄúMCP servers‚Äù for your app.

B) (Optional) Upgrade to official MCP protocol later

If you want to expose your tools to MCP-compatible clients (VS Code/Claude/other), the steps are:

Install the Python SDK:

pip install mcp


Create backend/mcp/server.py that wraps your tools:

Tools to expose:

list_files ‚Üí calls MCPFileOps.list_files()

read_file(name) ‚Üí calls MCPFileOps.read_file(name)

search_corpus(q,k) ‚Üí calls MCPSearch.search(q,k) (uses pgvector if enabled)

Run the server over stdio (typical for MCP): your main() calls server.run_stdio().

Since MCP SDK APIs change over time, keep this optional. Your current HTTP ‚ÄúMCP-like‚Äù endpoints already meet your project goals and integrate cleanly with your UI and logs.

3) Quick run checklist

Start Postgres/pgvector and create tables (step 1D).

Set .env:

USE_PGVECTOR=1
DATABASE_URL=postgresql://postgres:postgres@127.0.0.1:5432/docs
PGVECTOR_DIM=384


pip install -r requirements.txt (ensure it includes psycopg[binary] and pgvector).

Restart API (uvicorn backend.api.main:app --reload) and Streamlit.

Upload a doc ‚Üí it will be embedded & stored in Postgres.

Try /mcp/search?q=... to see vector results.

If you want, I can also give you a one-file initializer that:

pings the DB,

creates the tables/index if missing, and

migrates any already-uploaded documents from memory to pgvector.













 yes give it and also tell how to add the api keys to it ... all differnect keys to and where where i need to update
ChatGPT said:
Thought for 1m 26s

Awesome ‚Äî here‚Äôs a one‚Äìfile initializer that:

connects to Postgres,

ensures pgvector is enabled and tables / index exist,

scans your backend/data/uploads/ folder,

parses & chunks each file,

embeds and migrates it into Postgres using pgvector.

Then, below the script, I‚Äôve listed all API keys / env vars to add, exactly where they‚Äôre used, and a tiny SummaryAgent patch so OpenAI / Azure OpenAI keys work via env only.

üîß 1) One-shot initializer / migrator

Save as init_pgvector_and_migrate.py at your project root (intelligent-docs/) and run:

python init_pgvector_and_migrate.py

# init_pgvector_and_migrate.py
"""
Initializes pgvector DB and migrates existing uploads into Postgres.
- Creates tables & IVF cosine index if missing
- Parses & chunks every file in backend/data/uploads/
- Computes embeddings (same HashedEmbeddings as app) and inserts into pgvector

Run from project root:
    python init_pgvector_and_migrate.py

Requires in your environment (or .env):
    DATABASE_URL=postgresql://USER:PASS@HOST:5432/docs
    PGVECTOR_DIM=384
"""

from __future__ import annotations
import os
import sys
import math
import hashlib
from pathlib import Path
from typing import List, Tuple

# --- make project importable ---
ROOT = Path(__file__).resolve().parent
sys.path.append(str(ROOT))

# Load settings (this also reads .env via pydantic_settings in your app)
from backend.core.config import settings  # type: ignore

# Use your existing lightweight stack
import psycopg  # type: ignore
from backend.services.parser import parse_file  # type: ignore
from backend.services.chunker import chunk_text  # type: ignore
from backend.services.embeddings import HashedEmbeddings  # type: ignore

# --------- helpers ---------
def l2_normalize(v: List[float]) -> List[float]:
    s = math.sqrt(sum(x*x for x in v)) or 1.0
    return [x / s for x in v]

def vec_literal(v: List[float]) -> str:
    return "[" + ",".join(f"{x:.6f}" for x in v) + "]"

def sha1_id(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def ensure_schema(conn, dim: int) -> None:
    SQL = f"""
    CREATE EXTENSION IF NOT EXISTS vector;

    CREATE TABLE IF NOT EXISTS documents (
      id TEXT PRIMARY KEY,
      filename TEXT NOT NULL,
      path TEXT NOT NULL
    );

    CREATE TABLE IF NOT EXISTS chunks (
      doc_id TEXT NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
      chunk_idx INT NOT NULL,
      content TEXT NOT NULL,
      embedding VECTOR({dim}) NOT NULL,
      PRIMARY KEY (doc_id, chunk_idx)
    );

    CREATE INDEX IF NOT EXISTS idx_chunks_embedding_ivf
      ON chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
    """
    with conn.cursor() as cur:
        cur.execute(SQL)
        cur.execute("ANALYZE chunks;")

def migrate_file(conn, dim: int, embedder: HashedEmbeddings, path: Path) -> Tuple[str, int]:
    ext = path.suffix.lower()
    # parse
    text = parse_file(str(path), ext)
    # chunk
    chunks = chunk_text(text)
    # embed
    embs = [l2_normalize(list(map(float, e))) for e in embedder.embed_texts(chunks)]

    # derive a stable id from filename + size + mtime (so reruns upsert predictably)
    stat = path.stat()
    doc_id = sha1_id(f"{path.name}|{stat.st_size}|{int(stat.st_mtime)}")

    with conn.cursor() as cur:
        # upsert documents row
        cur.execute(
            "INSERT INTO documents (id, filename, path) VALUES (%s, %s, %s) "
            "ON CONFLICT (id) DO UPDATE SET filename=EXCLUDED.filename, path=EXCLUDED.path",
            (doc_id, path.name, str(path)),
        )
        # clear existing chunks for this doc
        cur.execute("DELETE FROM chunks WHERE doc_id=%s", (doc_id,))
        # insert chunks
        for i, (content, emb) in enumerate(zip(chunks, embs)):
            cur.execute(
                "INSERT INTO chunks (doc_id, chunk_idx, content, embedding) VALUES (%s, %s, %s, %s::vector)",
                (doc_id, i, content, vec_literal(emb)),
            )

    return doc_id, len(chunks)

def main():
    dsn = os.getenv("DATABASE_URL", getattr(settings, "database_url", None))
    if not dsn:
        print("‚ùå DATABASE_URL is not set. Put it in .env or your environment.")
        print("   Example: postgresql://postgres:postgres@127.0.0.1:5432/docs")
        sys.exit(1)

    dim = int(os.getenv("PGVECTOR_DIM", "384"))

    # connect
    try:
        conn = psycopg.connect(dsn, autocommit=True)
    except Exception as e:
        print("‚ùå Could not connect to Postgres:", e)
        sys.exit(1)

    print("‚úÖ Connected to Postgres.")
    ensure_schema(conn, dim)
    print("‚úÖ Schema ensured (tables + vector index).")

    uploads = ROOT / "backend" / "data" / "uploads"
    uploads.mkdir(parents=True, exist_ok=True)

    files = [p for p in uploads.iterdir() if p.is_file() and p.suffix.lower() in {".pdf",".docx",".txt",".html",".htm"}]
    if not files:
        print("‚ÑπÔ∏è No files found in backend/data/uploads/. Nothing to migrate.")
        return

    embedder = HashedEmbeddings(dim=dim)
    migrated = 0
    for p in files:
        try:
            doc_id, n_chunks = migrate_file(conn, dim, embedder, p)
            print(f"  ‚Ä¢ {p.name}  ‚Üí doc_id={doc_id[:8]}‚Ä¶  chunks={n_chunks}")
            migrated += 1
        except Exception as e:
            print(f"  ‚úñ Failed: {p.name}: {e}")

    print(f"\n‚úÖ Migration complete. Files processed: {migrated}/{len(files)}")
    print("You can now start the API with USE_PGVECTOR=1 to query Postgres-backed embeddings.")

if __name__ == "__main__":
    main()

üîë 2) API keys & environment variables

Put these in your .env (keep a sanitized copy in .env.example and commit only the example).

Core app / server
APP_ENV=dev
API_HOST=127.0.0.1
API_PORT=8000
CORS_ORIGINS=*
SUMMARY_WORDS_DEFAULT=350

pgvector (persistent embeddings)
USE_PGVECTOR=1
DATABASE_URL=postgresql://postgres:postgres@127.0.0.1:5432/docs
PGVECTOR_DIM=384


PGVECTOR_DIM must match your embedder dimension (the project uses 384).

LLM (OpenAI or Azure OpenAI) ‚Äî optional, for abstractive summaries
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx

# Optional overrides:
OPENAI_MODEL=gpt-4o-mini          # override the default
OPENAI_BASE_URL=                  # e.g. for Azure: https://<resource>.openai.azure.com/openai
OPENAI_API_VERSION=2024-06-01     # only needed for Azure; leave blank for OpenAI


If you‚Äôre using Azure OpenAI:

Keep LLM_PROVIDER=openai.

Set OPENAI_BASE_URL to your Azure endpoint‚Äôs ‚Äú/openai‚Äù root.

Set OPENAI_API_VERSION to your deployed API version.

Set OPENAI_MODEL to the deployment name (not the model family).

(Optional) Observability / LangSmith

Your code doesn‚Äôt push to LangSmith yet, but you can pre-wire env:

LANGSMITH_API_KEY=
LANGSMITH_PROJECT=intelligent-docs

üß© 3) Tiny SummaryAgent tweak (so keys work without code changes later)

Add this small patch to backend/services/summary_agent.py (inside __init__):

self.provider = (os.getenv("LLM_PROVIDER") or "").lower()
self.openai_key = os.getenv("OPENAI_API_KEY")
self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
base_url = os.getenv("OPENAI_BASE_URL") or None
api_version = os.getenv("OPENAI_API_VERSION") or None

self._client = None
if self.provider == "openai" and self.openai_key:
    try:
        from openai import OpenAI
        # Works for OpenAI and Azure OpenAI (with base_url + api-version)
        kwargs = {"api_key": self.openai_key}
        if base_url:
            kwargs["base_url"] = base_url
        if api_version:
            kwargs["default_query"] = {"api-version": api_version}
        self._client = OpenAI(**kwargs)
    except Exception:
        self._client = None


‚Ä¶and when you create the completion, just use:

resp = self._client.chat.completions.create(
    model=self.model,               # uses OPENAI_MODEL env or default
    messages=[...],
    temperature=temperature,
)


This way:

For OpenAI: set only OPENAI_API_KEY (and optionally OPENAI_MODEL).

For Azure OpenAI: set OPENAI_API_KEY, OPENAI_BASE_URL, OPENAI_API_VERSION, and OPENAI_MODEL (deployment name).

üöÄ 4) Run order (end-to-end)

Start Postgres (with vector extension), then run the initializer:

python init_pgvector_and_migrate.py


Add to .env:

USE_PGVECTOR=1
DATABASE_URL=postgresql://postgres:postgres@127.0.0.1:5432/docs
PGVECTOR_DIM=384
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
# (Azure users) OPENAI_BASE_URL=..., OPENAI_API_VERSION=..., OPENAI_MODEL=<deployment>


Start servers:

uvicorn backend.api.main:app --reload --host %API_HOST% --port %API_PORT%
streamlit run ui/app.py


Upload files / regenerate summaries / ask Q&A as normal ‚Äî now backed by pgvector.

If you want, I can also give you a reverse migrator (pgvector ‚Üí JSON dumps) for backups, but the above is everything you need to get persistent embeddings + keys wired up cleanly.